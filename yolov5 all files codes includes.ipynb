{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46520261",
   "metadata": {},
   "source": [
    "# requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt\n",
    "\n",
    "# Base ---------------------------------------\n",
    "-\n",
    "matplotlib>=3.2.2\n",
    "\n",
    "numpy>=1.18.5\n",
    "opencv\n",
    "-python>=4.1.2\n",
    "\n",
    "Pillow>=7.1.2\n",
    "\n",
    "PyYAML>=5.3.1\n",
    "\n",
    "requests>=2.23.0\n",
    "\n",
    "scipy>=1.4.1\n",
    "\n",
    "torch>=1.7.0\n",
    "\n",
    "torchvision>=0.8.1\n",
    "\n",
    "tqdm>=4.41.0\n",
    "\n",
    "\n",
    "# Logging -------------------------------------\n",
    "tensorboard>=2.4.1\n",
    "#\n",
    " wandb\n",
    "\n",
    "# Plotting ------------------------------------\n",
    "pandas>=1.1.4\n",
    "\n",
    "seaborn>=0.11.0\n",
    "\n",
    "# Export --------------------------------------\n",
    "# coremltools>=4.1 \n",
    " # CoreML export\n",
    "# onnx>=1.9.0  # ONNX export\n",
    "# onnx-simplifier>=0.3.6\n",
    "  # ONNX simplifier\n",
    "# scikit-learn==0.19.2 \n",
    " # CoreML quantization\n",
    "# tensorflow>=2.4.1 \n",
    " # TFLite export\n",
    "# tensorflowjs>=3.9.0  \n",
    "# TF.js export\n",
    "# openvino-dev \n",
    " # OpenVINO export\n",
    "\n",
    "\n",
    "# Extras --------------------------------------\n",
    "# albumentations>=1.0.3\n",
    "\n",
    "# Cython  # for pycocotools https://github.com/cocodataset/cocoapi/issues/172\n",
    "\n",
    "# pycocotools>=2.0  # COCO mAP\n",
    "# roboflow\n",
    "thop  # FLOPs computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe519d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e8204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fed032b3",
   "metadata": {},
   "source": [
    "# detect.py file code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23229dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Run inference on images, videos, directories, streams, etc.\n",
    "\n",
    "Usage - sources:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt --source 0              # webcam\n",
    "                                                             img.jpg        # image\n",
    "                                                             vid.mp4        # video\n",
    "                                                             path/          # directory\n",
    "                                                             path/*.jpg     # glob\n",
    "                                                             'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
    "                                                             'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "\n",
    "Usage - formats:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch\n",
    "                                         yolov5s.torchscript        # TorchScript\n",
    "                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
    "                                         yolov5s.xml                # OpenVINO\n",
    "                                         yolov5s.engine             # TensorRT\n",
    "                                         yolov5s.mlmodel            # CoreML (MacOS-only)\n",
    "                                         yolov5s_saved_model        # TensorFlow SavedModel\n",
    "                                         yolov5s.pb                 # TensorFlow GraphDef\n",
    "                                         yolov5s.tflite             # TensorFlow Lite\n",
    "                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
    "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run(weights=ROOT / 'yolov5s.pt',  # model.pt path(s)\n",
    "        source=ROOT / 'data/images',  # file/dir/URL/glob, 0 for webcam\n",
    "        data=ROOT / 'data/coco128.yaml',  # dataset.yaml path\n",
    "        imgsz=(640, 640),  # inference size (height, width)\n",
    "        conf_thres=0.25,  # confidence threshold\n",
    "        iou_thres=0.45,  # NMS IOU threshold\n",
    "        max_det=1000,  # maximum detections per image\n",
    "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        view_img=False,  # show results\n",
    "        save_txt=False,  # save results to *.txt\n",
    "        save_conf=False,  # save confidences in --save-txt labels\n",
    "        save_crop=False,  # save cropped prediction boxes\n",
    "        nosave=False,  # do not save images/videos\n",
    "        classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
    "        agnostic_nms=False,  # class-agnostic NMS\n",
    "        augment=False,  # augmented inference\n",
    "        visualize=False,  # visualize features\n",
    "        update=False,  # update all models\n",
    "        project=ROOT / 'runs/detect',  # save results to project/name\n",
    "        name='exp',  # save results to project/name\n",
    "        exist_ok=False,  # existing project/name ok, do not increment\n",
    "        line_thickness=3,  # bounding box thickness (pixels)\n",
    "        hide_labels=False,  # hide labels\n",
    "        hide_conf=False,  # hide confidences\n",
    "        half=False,  # use FP16 half-precision inference\n",
    "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
    "        ):\n",
    "    source = str(source)\n",
    "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n",
    "    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)\n",
    "    if is_url and is_file:\n",
    "        source = check_file(source)  # download\n",
    "\n",
    "    # Directories\n",
    "    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Load model\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n",
    "    stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "    # Half\n",
    "    half &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16 supported on limited backends with CUDA\n",
    "    if pt or jit:\n",
    "        model.model.half() if half else model.model.float()\n",
    "\n",
    "    # Dataloader\n",
    "    if webcam:\n",
    "        view_img = check_imshow()\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = len(dataset)  # batch_size\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "        bs = 1  # batch_size\n",
    "    vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "    # Run inference\n",
    "    model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup\n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        t1 = time_sync()\n",
    "        im = torch.from_numpy(im).to(device)\n",
    "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        if len(im.shape) == 3:\n",
    "            im = im[None]  # expand for batch dim\n",
    "        t2 = time_sync()\n",
    "        dt[0] += t2 - t1\n",
    "\n",
    "        # Inference\n",
    "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n",
    "        pred = model(im, augment=augment, visualize=visualize)\n",
    "        t3 = time_sync()\n",
    "        dt[1] += t3 - t2\n",
    "\n",
    "        # NMS\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "        dt[2] += time_sync() - t3\n",
    "\n",
    "        # Second-stage classifier (optional)\n",
    "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
    "\n",
    "        # Process predictions\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            seen += 1\n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n",
    "                s += f'{i}: '\n",
    "            else:\n",
    "                p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            save_path = str(save_dir / p.name)  # im.jpg\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "            s += '%gx%g ' % im.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                        with open(txt_path + '.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                        c = int(cls)  # integer class\n",
    "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                        if save_crop:\n",
    "                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "            # Print time (inference-only)\n",
    "            LOGGER.info(f'{s}Done. ({t3 - t2:.3f}s)')\n",
    "\n",
    "            # Stream results\n",
    "            im0 = annotator.result()\n",
    "            if view_img:\n",
    "                cv2.imshow(str(p), im0)\n",
    "                cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path[i] != save_path:  # new video\n",
    "                        vid_path[i] = save_path\n",
    "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                            vid_writer[i].release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
    "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                    vid_writer[i].write(im0)\n",
    "\n",
    "    # Print results\n",
    "    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n",
    "    LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}' % t)\n",
    "    if save_txt or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "    if update:\n",
    "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
    "    parser.add_argument('--source', type=str, default=ROOT / 'data/images', help='file/dir/URL/glob, 0 for webcam')\n",
    "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640], help='inference size h,w')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.25, help='confidence threshold')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.45, help='NMS IoU threshold')\n",
    "    parser.add_argument('--max-det', type=int, default=1000, help='maximum detections per image')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--view-img', action='store_true', help='show results')\n",
    "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "    parser.add_argument('--save-crop', action='store_true', help='save cropped prediction boxes')\n",
    "    parser.add_argument('--nosave', action='store_true', help='do not save images/videos')\n",
    "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n",
    "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "    parser.add_argument('--visualize', action='store_true', help='visualize features')\n",
    "    parser.add_argument('--update', action='store_true', help='update all models')\n",
    "    parser.add_argument('--project', default=ROOT / 'runs/detect', help='save results to project/name')\n",
    "    parser.add_argument('--name', default='exp', help='save results to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--line-thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
    "    parser.add_argument('--hide-labels', default=False, action='store_true', help='hide labels')\n",
    "    parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
    "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
    "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
    "    opt = parser.parse_args()\n",
    "    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n",
    "    print_args(FILE.stem, opt)\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    check_requirements(exclude=('tensorboard', 'thop'))\n",
    "    run(**vars(opt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b32e6",
   "metadata": {},
   "source": [
    "# export.py file code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8124ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit\n",
    "\n",
    "Format                      | `export.py --include`         | Model\n",
    "---                         | ---                           | ---\n",
    "PyTorch                     | -                             | yolov5s.pt\n",
    "TorchScript                 | `torchscript`                 | yolov5s.torchscript\n",
    "ONNX                        | `onnx`                        | yolov5s.onnx\n",
    "OpenVINO                    | `openvino`                    | yolov5s_openvino_model/\n",
    "TensorRT                    | `engine`                      | yolov5s.engine\n",
    "CoreML                      | `coreml`                      | yolov5s.mlmodel\n",
    "TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/\n",
    "TensorFlow GraphDef         | `pb`                          | yolov5s.pb\n",
    "TensorFlow Lite             | `tflite`                      | yolov5s.tflite\n",
    "TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite\n",
    "TensorFlow.js               | `tfjs`                        | yolov5s_web_model/\n",
    "\n",
    "Usage:\n",
    "    $ python path/to/export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...\n",
    "\n",
    "Inference:\n",
    "    $ python path/to/detect.py --weights yolov5s.pt                 # PyTorch\n",
    "                                         yolov5s.torchscript        # TorchScript\n",
    "                                         yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
    "                                         yolov5s.xml                # OpenVINO\n",
    "                                         yolov5s.engine             # TensorRT\n",
    "                                         yolov5s.mlmodel            # CoreML (MacOS-only)\n",
    "                                         yolov5s_saved_model        # TensorFlow SavedModel\n",
    "                                         yolov5s.pb                 # TensorFlow GraphDef\n",
    "                                         yolov5s.tflite             # TensorFlow Lite\n",
    "                                         yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
    "\n",
    "TensorFlow.js:\n",
    "    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example\n",
    "    $ npm install\n",
    "    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model\n",
    "    $ npm start\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from models.common import Conv\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Detect\n",
    "from utils.activations import SiLU\n",
    "from utils.datasets import LoadImages\n",
    "from utils.general import (LOGGER, check_dataset, check_img_size, check_requirements, check_version, colorstr,\n",
    "                           file_size, print_args, url2file)\n",
    "from utils.torch_utils import select_device\n",
    "\n",
    "\n",
    "def export_torchscript(model, im, file, optimize, prefix=colorstr('TorchScript:')):\n",
    "    # YOLOv5 TorchScript model export\n",
    "    try:\n",
    "        LOGGER.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n",
    "        f = file.with_suffix('.torchscript')\n",
    "\n",
    "        ts = torch.jit.trace(model, im, strict=False)\n",
    "        d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n",
    "        extra_files = {'config.txt': json.dumps(d)}  # torch._C.ExtraFilesMap()\n",
    "        if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n",
    "            optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n",
    "        else:\n",
    "            ts.save(str(f), _extra_files=extra_files)\n",
    "\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "def export_onnx(model, im, file, opset, train, dynamic, simplify, prefix=colorstr('ONNX:')):\n",
    "    # YOLOv5 ONNX export\n",
    "    try:\n",
    "        check_requirements(('onnx',))\n",
    "        import onnx\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with onnx {onnx.__version__}...')\n",
    "        f = file.with_suffix('.onnx')\n",
    "\n",
    "        torch.onnx.export(model, im, f, verbose=False, opset_version=opset,\n",
    "                          training=torch.onnx.TrainingMode.TRAINING if train else torch.onnx.TrainingMode.EVAL,\n",
    "                          do_constant_folding=not train,\n",
    "                          input_names=['images'],\n",
    "                          output_names=['output'],\n",
    "                          dynamic_axes={'images': {0: 'batch', 2: 'height', 3: 'width'},  # shape(1,3,640,640)\n",
    "                                        'output': {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)\n",
    "                                        } if dynamic else None)\n",
    "\n",
    "        # Checks\n",
    "        model_onnx = onnx.load(f)  # load onnx model\n",
    "        onnx.checker.check_model(model_onnx)  # check onnx model\n",
    "        # LOGGER.info(onnx.helper.printable_graph(model_onnx.graph))  # print\n",
    "\n",
    "        # Simplify\n",
    "        if simplify:\n",
    "            try:\n",
    "                check_requirements(('onnx-simplifier',))\n",
    "                import onnxsim\n",
    "\n",
    "                LOGGER.info(f'{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...')\n",
    "                model_onnx, check = onnxsim.simplify(\n",
    "                    model_onnx,\n",
    "                    dynamic_input_shape=dynamic,\n",
    "                    input_shapes={'images': list(im.shape)} if dynamic else None)\n",
    "                assert check, 'assert check failed'\n",
    "                onnx.save(model_onnx, f)\n",
    "            except Exception as e:\n",
    "                LOGGER.info(f'{prefix} simplifier failure: {e}')\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "def export_openvino(model, im, file, prefix=colorstr('OpenVINO:')):\n",
    "    # YOLOv5 OpenVINO export\n",
    "    try:\n",
    "        check_requirements(('openvino-dev',))  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n",
    "        import openvino.inference_engine as ie\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with openvino {ie.__version__}...')\n",
    "        f = str(file).replace('.pt', '_openvino_model' + os.sep)\n",
    "\n",
    "        cmd = f\"mo --input_model {file.with_suffix('.onnx')} --output_dir {f}\"\n",
    "        subprocess.check_output(cmd, shell=True)\n",
    "\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "def export_coreml(model, im, file, prefix=colorstr('CoreML:')):\n",
    "    # YOLOv5 CoreML export\n",
    "    try:\n",
    "        check_requirements(('coremltools',))\n",
    "        import coremltools as ct\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with coremltools {ct.__version__}...')\n",
    "        f = file.with_suffix('.mlmodel')\n",
    "\n",
    "        ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n",
    "        ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im.shape, scale=1 / 255, bias=[0, 0, 0])])\n",
    "        ct_model.save(f)\n",
    "\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return ct_model, f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def export_engine(model, im, file, train, half, simplify, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n",
    "    # YOLOv5 TensorRT export https://developer.nvidia.com/tensorrt\n",
    "    try:\n",
    "        check_requirements(('tensorrt',))\n",
    "        import tensorrt as trt\n",
    "\n",
    "        if trt.__version__[0] == '7':  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n",
    "            grid = model.model[-1].anchor_grid\n",
    "            model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n",
    "            export_onnx(model, im, file, 12, train, False, simplify)  # opset 12\n",
    "            model.model[-1].anchor_grid = grid\n",
    "        else:  # TensorRT >= 8\n",
    "            check_version(trt.__version__, '8.0.0', hard=True)  # require tensorrt>=8.0.0\n",
    "            export_onnx(model, im, file, 13, train, False, simplify)  # opset 13\n",
    "        onnx = file.with_suffix('.onnx')\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with TensorRT {trt.__version__}...')\n",
    "        assert im.device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n",
    "        assert onnx.exists(), f'failed to export ONNX file: {onnx}'\n",
    "        f = file.with_suffix('.engine')  # TensorRT engine file\n",
    "        logger = trt.Logger(trt.Logger.INFO)\n",
    "        if verbose:\n",
    "            logger.min_severity = trt.Logger.Severity.VERBOSE\n",
    "\n",
    "        builder = trt.Builder(logger)\n",
    "        config = builder.create_builder_config()\n",
    "        config.max_workspace_size = workspace * 1 << 30\n",
    "\n",
    "        flag = (1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "        network = builder.create_network(flag)\n",
    "        parser = trt.OnnxParser(network, logger)\n",
    "        if not parser.parse_from_file(str(onnx)):\n",
    "            raise RuntimeError(f'failed to load ONNX file: {onnx}')\n",
    "\n",
    "        inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "        outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
    "        LOGGER.info(f'{prefix} Network Description:')\n",
    "        for inp in inputs:\n",
    "            LOGGER.info(f'{prefix}\\tinput \"{inp.name}\" with shape {inp.shape} and dtype {inp.dtype}')\n",
    "        for out in outputs:\n",
    "            LOGGER.info(f'{prefix}\\toutput \"{out.name}\" with shape {out.shape} and dtype {out.dtype}')\n",
    "\n",
    "        half &= builder.platform_has_fast_fp16\n",
    "        LOGGER.info(f'{prefix} building FP{16 if half else 32} engine in {f}')\n",
    "        if half:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "        with builder.build_engine(network, config) as engine, open(f, 'wb') as t:\n",
    "            t.write(engine.serialize())\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "def export_saved_model(model, im, file, dynamic,\n",
    "                       tf_nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45,\n",
    "                       conf_thres=0.25, prefix=colorstr('TensorFlow SavedModel:')):\n",
    "    # YOLOv5 TensorFlow SavedModel export\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow import keras\n",
    "\n",
    "        from models.tf import TFDetect, TFModel\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
    "        f = str(file).replace('.pt', '_saved_model')\n",
    "        batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
    "\n",
    "        tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)\n",
    "        im = tf.zeros((batch_size, *imgsz, 3))  # BHWC order for TensorFlow\n",
    "        y = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
    "        inputs = keras.Input(shape=(*imgsz, 3), batch_size=None if dynamic else batch_size)\n",
    "        outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n",
    "        keras_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        keras_model.trainable = False\n",
    "        keras_model.summary()\n",
    "        keras_model.save(f, save_format='tf')\n",
    "\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return keras_model, f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def export_pb(keras_model, im, file, prefix=colorstr('TensorFlow GraphDef:')):\n",
    "    # YOLOv5 TensorFlow GraphDef *.pb export https://github.com/leimao/Frozen_Graph_TensorFlow\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
    "        f = file.with_suffix('.pb')\n",
    "\n",
    "        m = tf.function(lambda x: keras_model(x))  # full model\n",
    "        m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n",
    "        frozen_func = convert_variables_to_constants_v2(m)\n",
    "        frozen_func.graph.as_graph_def()\n",
    "        tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n",
    "\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "def export_tflite(keras_model, im, file, int8, data, ncalib, prefix=colorstr('TensorFlow Lite:')):\n",
    "    # YOLOv5 TensorFlow Lite export\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with tensorflow {tf.__version__}...')\n",
    "        batch_size, ch, *imgsz = list(im.shape)  # BCHW\n",
    "        f = str(file).replace('.pt', '-fp16.tflite')\n",
    "\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        if int8:\n",
    "            from models.tf import representative_dataset_gen\n",
    "            dataset = LoadImages(check_dataset(data)['train'], img_size=imgsz, auto=False)  # representative data\n",
    "            converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib)\n",
    "            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "            converter.target_spec.supported_types = []\n",
    "            converter.inference_input_type = tf.uint8  # or tf.int8\n",
    "            converter.inference_output_type = tf.uint8  # or tf.int8\n",
    "            converter.experimental_new_quantizer = False\n",
    "            f = str(file).replace('.pt', '-int8.tflite')\n",
    "\n",
    "        tflite_model = converter.convert()\n",
    "        open(f, \"wb\").write(tflite_model)\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "def export_edgetpu(keras_model, im, file, prefix=colorstr('Edge TPU:')):\n",
    "    # YOLOv5 Edge TPU export https://coral.ai/docs/edgetpu/models-intro/\n",
    "    try:\n",
    "        cmd = 'edgetpu_compiler --version'\n",
    "        help_url = 'https://coral.ai/docs/edgetpu/compiler/'\n",
    "        assert platform.system() == 'Linux', f'export only supported on Linux. See {help_url}'\n",
    "        if subprocess.run(cmd, shell=True).returncode != 0:\n",
    "            LOGGER.info(f'\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}')\n",
    "            for c in ['curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -',\n",
    "                      'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n",
    "                      'sudo apt-get update',\n",
    "                      'sudo apt-get install edgetpu-compiler']:\n",
    "                subprocess.run(c, shell=True, check=True)\n",
    "        ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with Edge TPU compiler {ver}...')\n",
    "        f = str(file).replace('.pt', '-int8_edgetpu.tflite')  # Edge TPU model\n",
    "        f_tfl = str(file).replace('.pt', '-int8.tflite')  # TFLite model\n",
    "\n",
    "        cmd = f\"edgetpu_compiler -s {f_tfl}\"\n",
    "        subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "def export_tfjs(keras_model, im, file, prefix=colorstr('TensorFlow.js:')):\n",
    "    # YOLOv5 TensorFlow.js export\n",
    "    try:\n",
    "        check_requirements(('tensorflowjs',))\n",
    "        import re\n",
    "\n",
    "        import tensorflowjs as tfjs\n",
    "\n",
    "        LOGGER.info(f'\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...')\n",
    "        f = str(file).replace('.pt', '_web_model')  # js dir\n",
    "        f_pb = file.with_suffix('.pb')  # *.pb path\n",
    "        f_json = f + '/model.json'  # *.json path\n",
    "\n",
    "        cmd = f'tensorflowjs_converter --input_format=tf_frozen_model ' \\\n",
    "              f'--output_node_names=\"Identity,Identity_1,Identity_2,Identity_3\" {f_pb} {f}'\n",
    "        subprocess.run(cmd, shell=True)\n",
    "\n",
    "        json = open(f_json).read()\n",
    "        with open(f_json, 'w') as j:  # sort JSON Identity_* in ascending order\n",
    "            subst = re.sub(\n",
    "                r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
    "                r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
    "                r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n",
    "                r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}',\n",
    "                r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n",
    "                r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n",
    "                r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n",
    "                r'\"Identity_3\": {\"name\": \"Identity_3\"}}}',\n",
    "                json)\n",
    "            j.write(subst)\n",
    "\n",
    "        LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')\n",
    "        return f\n",
    "    except Exception as e:\n",
    "        LOGGER.info(f'\\n{prefix} export failure: {e}')\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run(data=ROOT / 'data/coco128.yaml',  # 'dataset.yaml path'\n",
    "        weights=ROOT / 'yolov5s.pt',  # weights path\n",
    "        imgsz=(640, 640),  # image (height, width)\n",
    "        batch_size=1,  # batch size\n",
    "        device='cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        include=('torchscript', 'onnx'),  # include formats\n",
    "        half=False,  # FP16 half-precision export\n",
    "        inplace=False,  # set YOLOv5 Detect() inplace=True\n",
    "        train=False,  # model.train() mode\n",
    "        optimize=False,  # TorchScript: optimize for mobile\n",
    "        int8=False,  # CoreML/TF INT8 quantization\n",
    "        dynamic=False,  # ONNX/TF: dynamic axes\n",
    "        simplify=False,  # ONNX: simplify model\n",
    "        opset=12,  # ONNX: opset version\n",
    "        verbose=False,  # TensorRT: verbose log\n",
    "        workspace=4,  # TensorRT: workspace size (GB)\n",
    "        nms=False,  # TF: add NMS to model\n",
    "        agnostic_nms=False,  # TF: add agnostic NMS to model\n",
    "        topk_per_class=100,  # TF.js NMS: topk per class to keep\n",
    "        topk_all=100,  # TF.js NMS: topk for all classes to keep\n",
    "        iou_thres=0.45,  # TF.js NMS: IoU threshold\n",
    "        conf_thres=0.25  # TF.js NMS: confidence threshold\n",
    "        ):\n",
    "    t = time.time()\n",
    "    include = [x.lower() for x in include]\n",
    "    tf_exports = list(x in include for x in ('saved_model', 'pb', 'tflite', 'edgetpu', 'tfjs'))  # TensorFlow exports\n",
    "    file = Path(url2file(weights) if str(weights).startswith(('http:/', 'https:/')) else weights)\n",
    "\n",
    "    # Checks\n",
    "    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n",
    "    opset = 12 if ('openvino' in include) else opset  # OpenVINO requires opset <= 12\n",
    "\n",
    "    # Load PyTorch model\n",
    "    device = select_device(device)\n",
    "    assert not (device.type == 'cpu' and half), '--half only compatible with GPU export, i.e. use --device 0'\n",
    "    model = attempt_load(weights, map_location=device, inplace=True, fuse=True)  # load FP32 model\n",
    "    nc, names = model.nc, model.names  # number of classes, class names\n",
    "\n",
    "    # Input\n",
    "    gs = int(max(model.stride))  # grid size (max stride)\n",
    "    imgsz = [check_img_size(x, gs) for x in imgsz]  # verify img_size are gs-multiples\n",
    "    im = torch.zeros(batch_size, 3, *imgsz).to(device)  # image size(1,3,320,192) BCHW iDetection\n",
    "\n",
    "    # Update model\n",
    "    if half:\n",
    "        im, model = im.half(), model.half()  # to FP16\n",
    "    model.train() if train else model.eval()  # training mode = no Detect() layer grid construction\n",
    "    for k, m in model.named_modules():\n",
    "        if isinstance(m, Conv):  # assign export-friendly activations\n",
    "            if isinstance(m.act, nn.SiLU):\n",
    "                m.act = SiLU()\n",
    "        elif isinstance(m, Detect):\n",
    "            m.inplace = inplace\n",
    "            m.onnx_dynamic = dynamic\n",
    "            if hasattr(m, 'forward_export'):\n",
    "                m.forward = m.forward_export  # assign custom forward (optional)\n",
    "\n",
    "    for _ in range(2):\n",
    "        y = model(im)  # dry runs\n",
    "    LOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from {file} ({file_size(file):.1f} MB)\")\n",
    "\n",
    "    # Exports\n",
    "    f = [''] * 10  # exported filenames\n",
    "    if 'torchscript' in include:\n",
    "        f[0] = export_torchscript(model, im, file, optimize)\n",
    "    if 'engine' in include:  # TensorRT required before ONNX\n",
    "        f[1] = export_engine(model, im, file, train, half, simplify, workspace, verbose)\n",
    "    if ('onnx' in include) or ('openvino' in include):  # OpenVINO requires ONNX\n",
    "        f[2] = export_onnx(model, im, file, opset, train, dynamic, simplify)\n",
    "    if 'openvino' in include:\n",
    "        f[3] = export_openvino(model, im, file)\n",
    "    if 'coreml' in include:\n",
    "        _, f[4] = export_coreml(model, im, file)\n",
    "\n",
    "    # TensorFlow Exports\n",
    "    if any(tf_exports):\n",
    "        pb, tflite, edgetpu, tfjs = tf_exports[1:]\n",
    "        if int8 or edgetpu:  # TFLite --int8 bug https://github.com/ultralytics/yolov5/issues/5707\n",
    "            check_requirements(('flatbuffers==1.12',))  # required before `import tensorflow`\n",
    "        assert not (tflite and tfjs), 'TFLite and TF.js models must be exported separately, please pass only one type.'\n",
    "        model, f[5] = export_saved_model(model, im, file, dynamic, tf_nms=nms or agnostic_nms or tfjs,\n",
    "                                         agnostic_nms=agnostic_nms or tfjs, topk_per_class=topk_per_class,\n",
    "                                         topk_all=topk_all, conf_thres=conf_thres, iou_thres=iou_thres)  # keras model\n",
    "        if pb or tfjs:  # pb prerequisite to tfjs\n",
    "            f[6] = export_pb(model, im, file)\n",
    "        if tflite or edgetpu:\n",
    "            f[7] = export_tflite(model, im, file, int8=int8 or edgetpu, data=data, ncalib=100)\n",
    "        if edgetpu:\n",
    "            f[8] = export_edgetpu(model, im, file)\n",
    "        if tfjs:\n",
    "            f[9] = export_tfjs(model, im, file)\n",
    "\n",
    "    # Finish\n",
    "    f = [str(x) for x in f if x]  # filter out '' and None\n",
    "    LOGGER.info(f'\\nExport complete ({time.time() - t:.2f}s)'\n",
    "                f\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\n",
    "                f\"\\nVisualize with https://netron.app\"\n",
    "                f\"\\nDetect with `python detect.py --weights {f[-1]}`\"\n",
    "                f\" or `model = torch.hub.load('ultralytics/yolov5', 'custom', '{f[-1]}')\"\n",
    "                f\"\\nValidate with `python val.py --weights {f[-1]}`\")\n",
    "    return f  # return list of exported files/dirs\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[640, 640], help='image (h, w)')\n",
    "    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n",
    "    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n",
    "    parser.add_argument('--inplace', action='store_true', help='set YOLOv5 Detect() inplace=True')\n",
    "    parser.add_argument('--train', action='store_true', help='model.train() mode')\n",
    "    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile')\n",
    "    parser.add_argument('--int8', action='store_true', help='CoreML/TF INT8 quantization')\n",
    "    parser.add_argument('--dynamic', action='store_true', help='ONNX/TF: dynamic axes')\n",
    "    parser.add_argument('--simplify', action='store_true', help='ONNX: simplify model')\n",
    "    parser.add_argument('--opset', type=int, default=12, help='ONNX: opset version')\n",
    "    parser.add_argument('--verbose', action='store_true', help='TensorRT: verbose log')\n",
    "    parser.add_argument('--workspace', type=int, default=4, help='TensorRT: workspace size (GB)')\n",
    "    parser.add_argument('--nms', action='store_true', help='TF: add NMS to model')\n",
    "    parser.add_argument('--agnostic-nms', action='store_true', help='TF: add agnostic NMS to model')\n",
    "    parser.add_argument('--topk-per-class', type=int, default=100, help='TF.js NMS: topk per class to keep')\n",
    "    parser.add_argument('--topk-all', type=int, default=100, help='TF.js NMS: topk for all classes to keep')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.45, help='TF.js NMS: IoU threshold')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.25, help='TF.js NMS: confidence threshold')\n",
    "    parser.add_argument('--include', nargs='+',\n",
    "                        default=['torchscript', 'onnx'],\n",
    "                        help='torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs')\n",
    "    opt = parser.parse_args()\n",
    "    print_args(FILE.stem, opt)\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n",
    "        run(**vars(opt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22645296",
   "metadata": {},
   "source": [
    "# hubconf.py file code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19591c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "PyTorch Hub models https://pytorch.org/hub/ultralytics_yolov5/\n",
    "\n",
    "Usage:\n",
    "    import torch\n",
    "    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "    model = torch.hub.load('ultralytics/yolov5:master', 'custom', 'path/to/yolov5s.onnx')  # file from branch\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def _create(name, pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    \"\"\"Creates a specified YOLOv5 model\n",
    "\n",
    "    Arguments:\n",
    "        name (str): name of model, i.e. 'yolov5s'\n",
    "        pretrained (bool): load pretrained weights into the model\n",
    "        channels (int): number of input channels\n",
    "        classes (int): number of model classes\n",
    "        autoshape (bool): apply YOLOv5 .autoshape() wrapper to model\n",
    "        verbose (bool): print all information to screen\n",
    "        device (str, torch.device, None): device to use for model parameters\n",
    "\n",
    "    Returns:\n",
    "        YOLOv5 pytorch model\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    from models.common import AutoShape, DetectMultiBackend\n",
    "    from models.yolo import Model\n",
    "    from utils.downloads import attempt_download\n",
    "    from utils.general import check_requirements, intersect_dicts, set_logging\n",
    "    from utils.torch_utils import select_device\n",
    "\n",
    "    check_requirements(exclude=('tensorboard', 'thop', 'opencv-python'))\n",
    "    set_logging(verbose=verbose)\n",
    "\n",
    "    name = Path(name)\n",
    "    path = name.with_suffix('.pt') if name.suffix == '' else name  # checkpoint path\n",
    "    try:\n",
    "        device = select_device(('0' if torch.cuda.is_available() else 'cpu') if device is None else device)\n",
    "\n",
    "        if pretrained and channels == 3 and classes == 80:\n",
    "            model = DetectMultiBackend(path, device=device)  # download/load FP32 model\n",
    "            # model = models.experimental.attempt_load(path, map_location=device)  # download/load FP32 model\n",
    "        else:\n",
    "            cfg = list((Path(__file__).parent / 'models').rglob(f'{path.stem}.yaml'))[0]  # model.yaml path\n",
    "            model = Model(cfg, channels, classes)  # create model\n",
    "            if pretrained:\n",
    "                ckpt = torch.load(attempt_download(path), map_location=device)  # load\n",
    "                csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "                csd = intersect_dicts(csd, model.state_dict(), exclude=['anchors'])  # intersect\n",
    "                model.load_state_dict(csd, strict=False)  # load\n",
    "                if len(ckpt['model'].names) == classes:\n",
    "                    model.names = ckpt['model'].names  # set class names attribute\n",
    "        if autoshape:\n",
    "            model = AutoShape(model)  # for file/URI/PIL/cv2/np inputs and NMS\n",
    "        return model.to(device)\n",
    "\n",
    "    except Exception as e:\n",
    "        help_url = 'https://github.com/ultralytics/yolov5/issues/36'\n",
    "        s = f'{e}. Cache may be out of date, try `force_reload=True` or see {help_url} for help.'\n",
    "        raise Exception(s) from e\n",
    "\n",
    "\n",
    "def custom(path='path/to/model.pt', autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5 custom or local model\n",
    "    return _create(path, autoshape=autoshape, verbose=verbose, device=device)\n",
    "\n",
    "\n",
    "def yolov5n(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-nano model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5n', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5s(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-small model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5s', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-medium model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5m', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-large model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5l', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-xlarge model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5x', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-nano-P6 model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5n6', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-small-P6 model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5s6', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-medium-P6 model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5m6', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-large-P6 model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5l6', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "def yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
    "    # YOLOv5-xlarge-P6 model https://github.com/ultralytics/yolov5\n",
    "    return _create('yolov5x6', pretrained, channels, classes, autoshape, verbose, device)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = _create(name='yolov5s', pretrained=True, channels=3, classes=80, autoshape=True, verbose=True)  # pretrained\n",
    "    # model = custom(path='path/to/model.pt')  # custom\n",
    "\n",
    "    # Verify inference\n",
    "    from pathlib import Path\n",
    "\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    imgs = ['data/images/zidane.jpg',  # filename\n",
    "            Path('data/images/zidane.jpg'),  # Path\n",
    "            'https://ultralytics.com/images/zidane.jpg',  # URI\n",
    "            cv2.imread('data/images/bus.jpg')[:, :, ::-1],  # OpenCV\n",
    "            Image.open('data/images/bus.jpg'),  # PIL\n",
    "            np.zeros((320, 640, 3))]  # numpy\n",
    "\n",
    "    results = model(imgs, size=320)  # batched inference\n",
    "    results.print()\n",
    "    results.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623a204",
   "metadata": {},
   "source": [
    "# setup.conf file code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb433cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project-wide configuration file, can be used for package metadata and other toll configurations\n",
    "# Example usage: global configuration for PEP8 (via flake8) setting or default pytest arguments\n",
    "\n",
    "[metadata]\n",
    "license_file = LICENSE\n",
    "description-file = README.md\n",
    "\n",
    "\n",
    "[tool:pytest]\n",
    "norecursedirs =\n",
    "    .git\n",
    "    dist\n",
    "    build\n",
    "addopts =\n",
    "    --doctest-modules\n",
    "    --durations=25\n",
    "    --color=yes\n",
    "\n",
    "\n",
    "[flake8]\n",
    "max-line-length = 120\n",
    "exclude = .tox,*.egg,build,temp\n",
    "select = E,W,F\n",
    "doctests = True\n",
    "verbose = 2\n",
    "# https://pep8.readthedocs.io/en/latest/intro.html#error-codes\n",
    "format = pylint\n",
    "# see: https://www.flake8rules.com/\n",
    "ignore =\n",
    "    E731  # Do not assign a lambda expression, use a def\n",
    "    F405\n",
    "    E402\n",
    "    F841\n",
    "    E741\n",
    "    F821\n",
    "    E722\n",
    "    F401\n",
    "    W504\n",
    "    E127\n",
    "    W504\n",
    "    E231\n",
    "    E501\n",
    "    F403\n",
    "    E302\n",
    "    F541\n",
    "\n",
    "\n",
    "[isort]\n",
    "# https://pycqa.github.io/isort/docs/configuration/options.html\n",
    "line_length = 120\n",
    "multi_line_output = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6346db33",
   "metadata": {},
   "source": [
    "# train.py file code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Train a YOLOv5 model on a custom dataset.\n",
    "\n",
    "Models and datasets download automatically from the latest YOLOv5 release.\n",
    "Models: https://github.com/ultralytics/yolov5/tree/master/models\n",
    "Datasets: https://github.com/ultralytics/yolov5/tree/master/data\n",
    "Tutorial: https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data\n",
    "\n",
    "Usage:\n",
    "    $ python path/to/train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (RECOMMENDED)\n",
    "    $ python path/to/train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import SGD, Adam, AdamW, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "import val  # for end-of-epoch mAP\n",
    "from models.experimental import attempt_load\n",
    "from models.yolo import Model\n",
    "from utils.autoanchor import check_anchors\n",
    "from utils.autobatch import check_train_batch_size\n",
    "from utils.callbacks import Callbacks\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.downloads import attempt_download\n",
    "from utils.general import (LOGGER, check_dataset, check_file, check_git_status, check_img_size, check_requirements,\n",
    "                           check_suffix, check_yaml, colorstr, get_latest_run, increment_path, init_seeds,\n",
    "                           intersect_dicts, labels_to_class_weights, labels_to_image_weights, methods, one_cycle,\n",
    "                           print_args, print_mutation, strip_optimizer)\n",
    "from utils.loggers import Loggers\n",
    "from utils.loggers.wandb.wandb_utils import check_wandb_resume\n",
    "from utils.loss import ComputeLoss\n",
    "from utils.metrics import fitness\n",
    "from utils.plots import plot_evolve, plot_labels\n",
    "from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, select_device, torch_distributed_zero_first\n",
    "\n",
    "LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html\n",
    "RANK = int(os.getenv('RANK', -1))\n",
    "WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))\n",
    "\n",
    "\n",
    "def train(hyp,  # path/to/hyp.yaml or hyp dictionary\n",
    "          opt,\n",
    "          device,\n",
    "          callbacks\n",
    "          ):\n",
    "    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \\\n",
    "        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \\\n",
    "        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze\n",
    "\n",
    "    # Directories\n",
    "    w = save_dir / 'weights'  # weights dir\n",
    "    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "    last, best = w / 'last.pt', w / 'best.pt'\n",
    "\n",
    "    # Hyperparameters\n",
    "    if isinstance(hyp, str):\n",
    "        with open(hyp, errors='ignore') as f:\n",
    "            hyp = yaml.safe_load(f)  # load hyps dict\n",
    "    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\n",
    "\n",
    "    # Save run settings\n",
    "    if not evolve:\n",
    "        with open(save_dir / 'hyp.yaml', 'w') as f:\n",
    "            yaml.safe_dump(hyp, f, sort_keys=False)\n",
    "        with open(save_dir / 'opt.yaml', 'w') as f:\n",
    "            yaml.safe_dump(vars(opt), f, sort_keys=False)\n",
    "\n",
    "    # Loggers\n",
    "    data_dict = None\n",
    "    if RANK in [-1, 0]:\n",
    "        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\n",
    "        if loggers.wandb:\n",
    "            data_dict = loggers.wandb.data_dict\n",
    "            if resume:\n",
    "                weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp\n",
    "\n",
    "        # Register actions\n",
    "        for k in methods(loggers):\n",
    "            callbacks.register_action(k, callback=getattr(loggers, k))\n",
    "\n",
    "    # Config\n",
    "    plots = not evolve  # create plots\n",
    "    cuda = device.type != 'cpu'\n",
    "    init_seeds(1 + RANK)\n",
    "    with torch_distributed_zero_first(LOCAL_RANK):\n",
    "        data_dict = data_dict or check_dataset(data)  # check if None\n",
    "    train_path, val_path = data_dict['train'], data_dict['val']\n",
    "    nc = 1 if single_cls else int(data_dict['nc'])  # number of classes\n",
    "    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
    "    assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # check\n",
    "    is_coco = isinstance(val_path, str) and val_path.endswith('coco/val2017.txt')  # COCO dataset\n",
    "\n",
    "    # Model\n",
    "    check_suffix(weights, '.pt')  # check weights\n",
    "    pretrained = weights.endswith('.pt')\n",
    "    if pretrained:\n",
    "        with torch_distributed_zero_first(LOCAL_RANK):\n",
    "            weights = attempt_download(weights)  # download if not found locally\n",
    "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys\n",
    "        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
    "        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n",
    "        model.load_state_dict(csd, strict=False)  # load\n",
    "        LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report\n",
    "    else:\n",
    "        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "\n",
    "    # Freeze\n",
    "    freeze = [f'model.{x}.' for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # layers to freeze\n",
    "    for k, v in model.named_parameters():\n",
    "        v.requires_grad = True  # train all layers\n",
    "        if any(x in k for x in freeze):\n",
    "            LOGGER.info(f'freezing {k}')\n",
    "            v.requires_grad = False\n",
    "\n",
    "    # Image size\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # verify imgsz is gs-multiple\n",
    "\n",
    "    # Batch size\n",
    "    if RANK == -1 and batch_size == -1:  # single-GPU only, estimate best batch size\n",
    "        batch_size = check_train_batch_size(model, imgsz)\n",
    "        loggers.on_params_update({\"batch_size\": batch_size})\n",
    "\n",
    "    # Optimizer\n",
    "    nbs = 64  # nominal batch size\n",
    "    accumulate = max(round(nbs / batch_size), 1)  # accumulate loss before optimizing\n",
    "    hyp['weight_decay'] *= batch_size * accumulate / nbs  # scale weight_decay\n",
    "    LOGGER.info(f\"Scaled weight_decay = {hyp['weight_decay']}\")\n",
    "\n",
    "    g0, g1, g2 = [], [], []  # optimizer parameter groups\n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # bias\n",
    "            g2.append(v.bias)\n",
    "        if isinstance(v, nn.BatchNorm2d):  # weight (no decay)\n",
    "            g0.append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # weight (with decay)\n",
    "            g1.append(v.weight)\n",
    "\n",
    "    if opt.optimizer == 'Adam':\n",
    "        optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "    elif opt.optimizer == 'AdamW':\n",
    "        optimizer = AdamW(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
    "    else:\n",
    "        optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
    "\n",
    "    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})  # add g1 with weight_decay\n",
    "    optimizer.add_param_group({'params': g2})  # add g2 (biases)\n",
    "    LOGGER.info(f\"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups \"\n",
    "                f\"{len(g0)} weight (no decay), {len(g1)} weight, {len(g2)} bias\")\n",
    "    del g0, g1, g2\n",
    "\n",
    "    # Scheduler\n",
    "    if opt.linear_lr:\n",
    "        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear\n",
    "    else:\n",
    "        lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)\n",
    "\n",
    "    # EMA\n",
    "    ema = ModelEMA(model) if RANK in [-1, 0] else None\n",
    "\n",
    "    # Resume\n",
    "    start_epoch, best_fitness = 0, 0.0\n",
    "    if pretrained:\n",
    "        # Optimizer\n",
    "        if ckpt['optimizer'] is not None:\n",
    "            optimizer.load_state_dict(ckpt['optimizer'])\n",
    "            best_fitness = ckpt['best_fitness']\n",
    "\n",
    "        # EMA\n",
    "        if ema and ckpt.get('ema'):\n",
    "            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())\n",
    "            ema.updates = ckpt['updates']\n",
    "\n",
    "        # Epochs\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        if resume:\n",
    "            assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'\n",
    "        if epochs < start_epoch:\n",
    "            LOGGER.info(f\"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.\")\n",
    "            epochs += ckpt['epoch']  # finetune additional epochs\n",
    "\n",
    "        del ckpt, csd\n",
    "\n",
    "    # DP mode\n",
    "    if cuda and RANK == -1 and torch.cuda.device_count() > 1:\n",
    "        LOGGER.warning('WARNING: DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\\n'\n",
    "                       'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # SyncBatchNorm\n",
    "    if opt.sync_bn and cuda and RANK != -1:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
    "        LOGGER.info('Using SyncBatchNorm()')\n",
    "\n",
    "    # Trainloader\n",
    "    train_loader, dataset = create_dataloader(train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,\n",
    "                                              hyp=hyp, augment=True, cache=opt.cache, rect=opt.rect, rank=LOCAL_RANK,\n",
    "                                              workers=workers, image_weights=opt.image_weights, quad=opt.quad,\n",
    "                                              prefix=colorstr('train: '), shuffle=True)\n",
    "    mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # max label class\n",
    "    nb = len(train_loader)  # number of batches\n",
    "    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'\n",
    "\n",
    "    # Process 0\n",
    "    if RANK in [-1, 0]:\n",
    "        val_loader = create_dataloader(val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,\n",
    "                                       hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,\n",
    "                                       workers=workers, pad=0.5,\n",
    "                                       prefix=colorstr('val: '))[0]\n",
    "\n",
    "        if not resume:\n",
    "            labels = np.concatenate(dataset.labels, 0)\n",
    "            # c = torch.tensor(labels[:, 0])  # classes\n",
    "            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\n",
    "            # model._initialize_biases(cf.to(device))\n",
    "            if plots:\n",
    "                plot_labels(labels, names, save_dir)\n",
    "\n",
    "            # Anchors\n",
    "            if not opt.noautoanchor:\n",
    "                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
    "            model.half().float()  # pre-reduce anchor precision\n",
    "\n",
    "        callbacks.run('on_pretrain_routine_end')\n",
    "\n",
    "    # DDP mode\n",
    "    if cuda and RANK != -1:\n",
    "        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)\n",
    "\n",
    "    # Model attributes\n",
    "    nl = de_parallel(model).model[-1].nl  # number of detection layers (to scale hyps)\n",
    "    hyp['box'] *= 3 / nl  # scale to layers\n",
    "    hyp['cls'] *= nc / 80 * 3 / nl  # scale to classes and layers\n",
    "    hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers\n",
    "    hyp['label_smoothing'] = opt.label_smoothing\n",
    "    model.nc = nc  # attach number of classes to model\n",
    "    model.hyp = hyp  # attach hyperparameters to model\n",
    "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights\n",
    "    model.names = names\n",
    "\n",
    "    # Start training\n",
    "    t0 = time.time()\n",
    "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
    "    last_opt_step = -1\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "    scaler = amp.GradScaler(enabled=cuda)\n",
    "    stopper = EarlyStopping(patience=opt.patience)\n",
    "    compute_loss = ComputeLoss(model)  # init loss class\n",
    "    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\\n'\n",
    "                f'Using {train_loader.num_workers * WORLD_SIZE} dataloader workers\\n'\n",
    "                f\"Logging results to {colorstr('bold', save_dir)}\\n\"\n",
    "                f'Starting training for {epochs} epochs...')\n",
    "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "        model.train()\n",
    "\n",
    "        # Update image weights (optional, single-GPU only)\n",
    "        if opt.image_weights:\n",
    "            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # class weights\n",
    "            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\n",
    "            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\n",
    "\n",
    "        # Update mosaic border (optional)\n",
    "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\n",
    "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\n",
    "\n",
    "        mloss = torch.zeros(3, device=device)  # mean losses\n",
    "        if RANK != -1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        pbar = enumerate(train_loader)\n",
    "        LOGGER.info(('\\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))\n",
    "        if RANK in [-1, 0]:\n",
    "            pbar = tqdm(pbar, total=nb, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "        optimizer.zero_grad()\n",
    "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "            # Warmup\n",
    "            if ni <= nw:\n",
    "                xi = [0, nw]  # x interp\n",
    "                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "            # Multi-scale\n",
    "            if opt.multi_scale:\n",
    "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "                if sf != 1:\n",
    "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast(enabled=cuda):\n",
    "                pred = model(imgs)  # forward\n",
    "                loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
    "                if RANK != -1:\n",
    "                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode\n",
    "                if opt.quad:\n",
    "                    loss *= 4.\n",
    "\n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni - last_opt_step >= accumulate:\n",
    "                scaler.step(optimizer)  # optimizer.step\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema:\n",
    "                    ema.update(model)\n",
    "                last_opt_step = ni\n",
    "\n",
    "            # Log\n",
    "            if RANK in [-1, 0]:\n",
    "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
    "                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)\n",
    "                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (\n",
    "                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))\n",
    "                callbacks.run('on_train_batch_end', ni, model, imgs, targets, paths, plots, opt.sync_bn)\n",
    "                if callbacks.stop_training:\n",
    "                    return\n",
    "            # end batch ------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Scheduler\n",
    "        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers\n",
    "        scheduler.step()\n",
    "\n",
    "        if RANK in [-1, 0]:\n",
    "            # mAP\n",
    "            callbacks.run('on_train_epoch_end', epoch=epoch)\n",
    "            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\n",
    "            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop\n",
    "            if not noval or final_epoch:  # Calculate mAP\n",
    "                results, maps, _ = val.run(data_dict,\n",
    "                                           batch_size=batch_size // WORLD_SIZE * 2,\n",
    "                                           imgsz=imgsz,\n",
    "                                           model=ema.ema,\n",
    "                                           single_cls=single_cls,\n",
    "                                           dataloader=val_loader,\n",
    "                                           save_dir=save_dir,\n",
    "                                           plots=False,\n",
    "                                           callbacks=callbacks,\n",
    "                                           compute_loss=compute_loss)\n",
    "\n",
    "            # Update best mAP\n",
    "            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\n",
    "            if fi > best_fitness:\n",
    "                best_fitness = fi\n",
    "            log_vals = list(mloss) + list(results) + lr\n",
    "            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)\n",
    "\n",
    "            # Save model\n",
    "            if (not nosave) or (final_epoch and not evolve):  # if save\n",
    "                ckpt = {'epoch': epoch,\n",
    "                        'best_fitness': best_fitness,\n",
    "                        'model': deepcopy(de_parallel(model)).half(),\n",
    "                        'ema': deepcopy(ema.ema).half(),\n",
    "                        'updates': ema.updates,\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None,\n",
    "                        'date': datetime.now().isoformat()}\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(ckpt, last)\n",
    "                if best_fitness == fi:\n",
    "                    torch.save(ckpt, best)\n",
    "                if (epoch > 0) and (opt.save_period > 0) and (epoch % opt.save_period == 0):\n",
    "                    torch.save(ckpt, w / f'epoch{epoch}.pt')\n",
    "                del ckpt\n",
    "                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)\n",
    "\n",
    "            # Stop Single-GPU\n",
    "            if RANK == -1 and stopper(epoch=epoch, fitness=fi):\n",
    "                break\n",
    "\n",
    "            # Stop DDP TODO: known issues shttps://github.com/ultralytics/yolov5/pull/4576\n",
    "            # stop = stopper(epoch=epoch, fitness=fi)\n",
    "            # if RANK == 0:\n",
    "            #    dist.broadcast_object_list([stop], 0)  # broadcast 'stop' to all ranks\n",
    "\n",
    "        # Stop DPP\n",
    "        # with torch_distributed_zero_first(RANK):\n",
    "        # if stop:\n",
    "        #    break  # must break all DDP ranks\n",
    "\n",
    "        # end epoch ----------------------------------------------------------------------------------------------------\n",
    "    # end training -----------------------------------------------------------------------------------------------------\n",
    "    if RANK in [-1, 0]:\n",
    "        LOGGER.info(f'\\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.')\n",
    "        for f in last, best:\n",
    "            if f.exists():\n",
    "                strip_optimizer(f)  # strip optimizers\n",
    "                if f is best:\n",
    "                    LOGGER.info(f'\\nValidating {f}...')\n",
    "                    results, _, _ = val.run(data_dict,\n",
    "                                            batch_size=batch_size // WORLD_SIZE * 2,\n",
    "                                            imgsz=imgsz,\n",
    "                                            model=attempt_load(f, device).half(),\n",
    "                                            iou_thres=0.65 if is_coco else 0.60,  # best pycocotools results at 0.65\n",
    "                                            single_cls=single_cls,\n",
    "                                            dataloader=val_loader,\n",
    "                                            save_dir=save_dir,\n",
    "                                            save_json=is_coco,\n",
    "                                            verbose=True,\n",
    "                                            plots=True,\n",
    "                                            callbacks=callbacks,\n",
    "                                            compute_loss=compute_loss)  # val best model with plots\n",
    "                    if is_coco:\n",
    "                        callbacks.run('on_fit_epoch_end', list(mloss) + list(results) + lr, epoch, best_fitness, fi)\n",
    "\n",
    "        callbacks.run('on_train_end', last, best, plots, epoch, results)\n",
    "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_opt(known=False):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')\n",
    "    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\n",
    "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
    "    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch.yaml', help='hyperparameters path')\n",
    "    parser.add_argument('--epochs', type=int, default=300)\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')\n",
    "    parser.add_argument('--rect', action='store_true', help='rectangular training')\n",
    "    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')\n",
    "    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n",
    "    parser.add_argument('--noval', action='store_true', help='only validate final epoch')\n",
    "    parser.add_argument('--noautoanchor', action='store_true', help='disable AutoAnchor')\n",
    "    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')\n",
    "    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n",
    "    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='--cache images in \"ram\" (default) or \"disk\"')\n",
    "    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\n",
    "    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')\n",
    "    parser.add_argument('--optimizer', type=str, choices=['SGD', 'Adam', 'AdamW'], default='SGD', help='optimizer')\n",
    "    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\n",
    "    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\n",
    "    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name')\n",
    "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--quad', action='store_true', help='quad dataloader')\n",
    "    parser.add_argument('--linear-lr', action='store_true', help='linear LR')\n",
    "    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')\n",
    "    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')\n",
    "    parser.add_argument('--freeze', nargs='+', type=int, default=[0], help='Freeze layers: backbone=10, first3=0 1 2')\n",
    "    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')\n",
    "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\n",
    "\n",
    "    # Weights & Biases arguments\n",
    "    parser.add_argument('--entity', default=None, help='W&B: Entity')\n",
    "    parser.add_argument('--upload_dataset', nargs='?', const=True, default=False, help='W&B: Upload data, \"val\" option')\n",
    "    parser.add_argument('--bbox_interval', type=int, default=-1, help='W&B: Set bounding-box image logging interval')\n",
    "    parser.add_argument('--artifact_alias', type=str, default='latest', help='W&B: Version of dataset artifact to use')\n",
    "\n",
    "    opt = parser.parse_known_args()[0] if known else parser.parse_args()\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt, callbacks=Callbacks()):\n",
    "    # Checks\n",
    "    if RANK in [-1, 0]:\n",
    "        print_args(FILE.stem, opt)\n",
    "        check_git_status()\n",
    "        check_requirements(exclude=['thop'])\n",
    "\n",
    "    # Resume\n",
    "    if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # resume an interrupted run\n",
    "        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\n",
    "        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\n",
    "        with open(Path(ckpt).parent.parent / 'opt.yaml', errors='ignore') as f:\n",
    "            opt = argparse.Namespace(**yaml.safe_load(f))  # replace\n",
    "        opt.cfg, opt.weights, opt.resume = '', ckpt, True  # reinstate\n",
    "        LOGGER.info(f'Resuming training from {ckpt}')\n",
    "    else:\n",
    "        opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = \\\n",
    "            check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks\n",
    "        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\n",
    "        if opt.evolve:\n",
    "            opt.project = str(ROOT / 'runs/evolve')\n",
    "            opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume\n",
    "        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))\n",
    "\n",
    "    # DDP mode\n",
    "    device = select_device(opt.device, batch_size=opt.batch_size)\n",
    "    if LOCAL_RANK != -1:\n",
    "        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'\n",
    "        assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'\n",
    "        assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'\n",
    "        assert not opt.evolve, '--evolve argument is not compatible with DDP training'\n",
    "        torch.cuda.set_device(LOCAL_RANK)\n",
    "        device = torch.device('cuda', LOCAL_RANK)\n",
    "        dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\")\n",
    "\n",
    "    # Train\n",
    "    if not opt.evolve:\n",
    "        train(opt.hyp, opt, device, callbacks)\n",
    "        if WORLD_SIZE > 1 and RANK == 0:\n",
    "            LOGGER.info('Destroying process group... ')\n",
    "            dist.destroy_process_group()\n",
    "\n",
    "    # Evolve hyperparameters (optional)\n",
    "    else:\n",
    "        # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)\n",
    "        meta = {'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "                'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "                'momentum': (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1\n",
    "                'weight_decay': (1, 0.0, 0.001),  # optimizer weight decay\n",
    "                'warmup_epochs': (1, 0.0, 5.0),  # warmup epochs (fractions ok)\n",
    "                'warmup_momentum': (1, 0.0, 0.95),  # warmup initial momentum\n",
    "                'warmup_bias_lr': (1, 0.0, 0.2),  # warmup initial bias lr\n",
    "                'box': (1, 0.02, 0.2),  # box loss gain\n",
    "                'cls': (1, 0.2, 4.0),  # cls loss gain\n",
    "                'cls_pw': (1, 0.5, 2.0),  # cls BCELoss positive_weight\n",
    "                'obj': (1, 0.2, 4.0),  # obj loss gain (scale with pixels)\n",
    "                'obj_pw': (1, 0.5, 2.0),  # obj BCELoss positive_weight\n",
    "                'iou_t': (0, 0.1, 0.7),  # IoU training threshold\n",
    "                'anchor_t': (1, 2.0, 8.0),  # anchor-multiple threshold\n",
    "                'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)\n",
    "                'fl_gamma': (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "                'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)\n",
    "                'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\n",
    "                'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)\n",
    "                'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)\n",
    "                'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)\n",
    "                'scale': (1, 0.0, 0.9),  # image scale (+/- gain)\n",
    "                'shear': (1, 0.0, 10.0),  # image shear (+/- deg)\n",
    "                'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001\n",
    "                'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)\n",
    "                'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)\n",
    "                'mosaic': (1, 0.0, 1.0),  # image mixup (probability)\n",
    "                'mixup': (1, 0.0, 1.0),  # image mixup (probability)\n",
    "                'copy_paste': (1, 0.0, 1.0)}  # segment copy-paste (probability)\n",
    "\n",
    "        with open(opt.hyp, errors='ignore') as f:\n",
    "            hyp = yaml.safe_load(f)  # load hyps dict\n",
    "            if 'anchors' not in hyp:  # anchors commented in hyp.yaml\n",
    "                hyp['anchors'] = 3\n",
    "        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # only val/save final epoch\n",
    "        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices\n",
    "        evolve_yaml, evolve_csv = save_dir / 'hyp_evolve.yaml', save_dir / 'evolve.csv'\n",
    "        if opt.bucket:\n",
    "            os.system(f'gsutil cp gs://{opt.bucket}/evolve.csv {save_dir}')  # download evolve.csv if exists\n",
    "\n",
    "        for _ in range(opt.evolve):  # generations to evolve\n",
    "            if evolve_csv.exists():  # if evolve.csv exists: select best hyps and mutate\n",
    "                # Select parent(s)\n",
    "                parent = 'single'  # parent selection method: 'single' or 'weighted'\n",
    "                x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)\n",
    "                n = min(5, len(x))  # number of previous results to consider\n",
    "                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n",
    "                w = fitness(x) - fitness(x).min() + 1E-6  # weights (sum > 0)\n",
    "                if parent == 'single' or len(x) == 1:\n",
    "                    # x = x[random.randint(0, n - 1)]  # random selection\n",
    "                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n",
    "                elif parent == 'weighted':\n",
    "                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n",
    "\n",
    "                # Mutate\n",
    "                mp, s = 0.8, 0.2  # mutation probability, sigma\n",
    "                npr = np.random\n",
    "                npr.seed(int(time.time()))\n",
    "                g = np.array([meta[k][0] for k in hyp.keys()])  # gains 0-1\n",
    "                ng = len(meta)\n",
    "                v = np.ones(ng)\n",
    "                while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n",
    "                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n",
    "                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n",
    "                    hyp[k] = float(x[i + 7] * v[i])  # mutate\n",
    "\n",
    "            # Constrain to limits\n",
    "            for k, v in meta.items():\n",
    "                hyp[k] = max(hyp[k], v[1])  # lower limit\n",
    "                hyp[k] = min(hyp[k], v[2])  # upper limit\n",
    "                hyp[k] = round(hyp[k], 5)  # significant digits\n",
    "\n",
    "            # Train mutation\n",
    "            results = train(hyp.copy(), opt, device, callbacks)\n",
    "            callbacks = Callbacks()\n",
    "            # Write mutation results\n",
    "            print_mutation(results, hyp.copy(), save_dir, opt.bucket)\n",
    "\n",
    "        # Plot results\n",
    "        plot_evolve(evolve_csv)\n",
    "        LOGGER.info(f'Hyperparameter evolution finished\\n'\n",
    "                    f\"Results saved to {colorstr('bold', save_dir)}\\n\"\n",
    "                    f'Use best hyperparameters example: $ python train.py --hyp {evolve_yaml}')\n",
    "\n",
    "\n",
    "def run(**kwargs):\n",
    "    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')\n",
    "    opt = parse_opt(True)\n",
    "    for k, v in kwargs.items():\n",
    "        setattr(opt, k, v)\n",
    "    main(opt)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4adcb8c",
   "metadata": {},
   "source": [
    "# val.py file code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452baffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "\"\"\"\n",
    "Validate a trained YOLOv5 model accuracy on a custom dataset\n",
    "\n",
    "Usage:\n",
    "    $ python path/to/val.py --weights yolov5s.pt --data coco128.yaml --img 640\n",
    "\n",
    "Usage - formats:\n",
    "    $ python path/to/val.py --weights yolov5s.pt                 # PyTorch\n",
    "                                      yolov5s.torchscript        # TorchScript\n",
    "                                      yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n",
    "                                      yolov5s.xml                # OpenVINO\n",
    "                                      yolov5s.engine             # TensorRT\n",
    "                                      yolov5s.mlmodel            # CoreML (MacOS-only)\n",
    "                                      yolov5s_saved_model        # TensorFlow SavedModel\n",
    "                                      yolov5s.pb                 # TensorFlow GraphDef\n",
    "                                      yolov5s.tflite             # TensorFlow Lite\n",
    "                                      yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "FILE = Path(__file__).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.callbacks import Callbacks\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import (LOGGER, box_iou, check_dataset, check_img_size, check_requirements, check_yaml,\n",
    "                           coco80_to_coco91_class, colorstr, increment_path, non_max_suppression, print_args,\n",
    "                           scale_coords, xywh2xyxy, xyxy2xywh)\n",
    "from utils.metrics import ConfusionMatrix, ap_per_class\n",
    "from utils.plots import output_to_target, plot_images, plot_val_study\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "\n",
    "\n",
    "def save_one_txt(predn, save_conf, shape, file):\n",
    "    # Save one txt result\n",
    "    gn = torch.tensor(shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "    for *xyxy, conf, cls in predn.tolist():\n",
    "        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "        with open(file, 'a') as f:\n",
    "            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "\n",
    "def save_one_json(predn, jdict, path, class_map):\n",
    "    # Save one JSON result {\"image_id\": 42, \"category_id\": 18, \"bbox\": [258.15, 41.29, 348.26, 243.78], \"score\": 0.236}\n",
    "    image_id = int(path.stem) if path.stem.isnumeric() else path.stem\n",
    "    box = xyxy2xywh(predn[:, :4])  # xywh\n",
    "    box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
    "    for p, b in zip(predn.tolist(), box.tolist()):\n",
    "        jdict.append({'image_id': image_id,\n",
    "                      'category_id': class_map[int(p[5])],\n",
    "                      'bbox': [round(x, 3) for x in b],\n",
    "                      'score': round(p[4], 5)})\n",
    "\n",
    "\n",
    "def process_batch(detections, labels, iouv):\n",
    "    \"\"\"\n",
    "    Return correct predictions matrix. Both sets of boxes are in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n",
    "        labels (Array[M, 5]), class, x1, y1, x2, y2\n",
    "    Returns:\n",
    "        correct (Array[N, 10]), for 10 IoU levels\n",
    "    \"\"\"\n",
    "    correct = torch.zeros(detections.shape[0], iouv.shape[0], dtype=torch.bool, device=iouv.device)\n",
    "    iou = box_iou(labels[:, 1:], detections[:, :4])\n",
    "    x = torch.where((iou >= iouv[0]) & (labels[:, 0:1] == detections[:, 5]))  # IoU above threshold and classes match\n",
    "    if x[0].shape[0]:\n",
    "        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detection, iou]\n",
    "        if x[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "            # matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "        matches = torch.Tensor(matches).to(iouv.device)\n",
    "        correct[matches[:, 1].long()] = matches[:, 2:3] >= iouv\n",
    "    return correct\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run(data,\n",
    "        weights=None,  # model.pt path(s)\n",
    "        batch_size=32,  # batch size\n",
    "        imgsz=640,  # inference size (pixels)\n",
    "        conf_thres=0.001,  # confidence threshold\n",
    "        iou_thres=0.6,  # NMS IoU threshold\n",
    "        task='val',  # train, val, test, speed or study\n",
    "        device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "        workers=8,  # max dataloader workers (per RANK in DDP mode)\n",
    "        single_cls=False,  # treat as single-class dataset\n",
    "        augment=False,  # augmented inference\n",
    "        verbose=False,  # verbose output\n",
    "        save_txt=False,  # save results to *.txt\n",
    "        save_hybrid=False,  # save label+prediction hybrid results to *.txt\n",
    "        save_conf=False,  # save confidences in --save-txt labels\n",
    "        save_json=False,  # save a COCO-JSON results file\n",
    "        project=ROOT / 'runs/val',  # save to project/name\n",
    "        name='exp',  # save to project/name\n",
    "        exist_ok=False,  # existing project/name ok, do not increment\n",
    "        half=True,  # use FP16 half-precision inference\n",
    "        dnn=False,  # use OpenCV DNN for ONNX inference\n",
    "        model=None,\n",
    "        dataloader=None,\n",
    "        save_dir=Path(''),\n",
    "        plots=True,\n",
    "        callbacks=Callbacks(),\n",
    "        compute_loss=None,\n",
    "        ):\n",
    "    # Initialize/load model and set device\n",
    "    training = model is not None\n",
    "    if training:  # called by train.py\n",
    "        device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model\n",
    "\n",
    "        half &= device.type != 'cpu'  # half precision only supported on CUDA\n",
    "        model.half() if half else model.float()\n",
    "    else:  # called directly\n",
    "        device = select_device(device, batch_size=batch_size)\n",
    "\n",
    "        # Directories\n",
    "        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "        # Load model\n",
    "        model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\n",
    "        stride, pt, jit, onnx, engine = model.stride, model.pt, model.jit, model.onnx, model.engine\n",
    "        imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "        half &= (pt or jit or onnx or engine) and device.type != 'cpu'  # FP16 supported on limited backends with CUDA\n",
    "        if pt or jit:\n",
    "            model.model.half() if half else model.model.float()\n",
    "        elif engine:\n",
    "            batch_size = model.batch_size\n",
    "        else:\n",
    "            half = False\n",
    "            batch_size = 1  # export.py models default to batch-size 1\n",
    "            device = torch.device('cpu')\n",
    "            LOGGER.info(f'Forcing --batch-size 1 square inference shape(1,3,{imgsz},{imgsz}) for non-PyTorch backends')\n",
    "\n",
    "        # Data\n",
    "        data = check_dataset(data)  # check\n",
    "\n",
    "    # Configure\n",
    "    model.eval()\n",
    "    is_coco = isinstance(data.get('val'), str) and data['val'].endswith('coco/val2017.txt')  # COCO dataset\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "\n",
    "    # Dataloader\n",
    "    if not training:\n",
    "        model.warmup(imgsz=(1, 3, imgsz, imgsz), half=half)  # warmup\n",
    "        pad = 0.0 if task == 'speed' else 0.5\n",
    "        task = task if task in ('train', 'val', 'test') else 'val'  # path to train/val/test images\n",
    "        dataloader = create_dataloader(data[task], imgsz, batch_size, stride, single_cls, pad=pad, rect=pt,\n",
    "                                       workers=workers, prefix=colorstr(f'{task}: '))[0]\n",
    "\n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    names = {k: v for k, v in enumerate(model.names if hasattr(model, 'names') else model.module.names)}\n",
    "    class_map = coco80_to_coco91_class() if is_coco else list(range(1000))\n",
    "    s = ('%20s' + '%11s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    dt, p, r, f1, mp, mr, map50, map = [0.0, 0.0, 0.0], 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    loss = torch.zeros(3, device=device)\n",
    "    jdict, stats, ap, ap_class = [], [], [], []\n",
    "    pbar = tqdm(dataloader, desc=s, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')  # progress bar\n",
    "    for batch_i, (im, targets, paths, shapes) in enumerate(pbar):\n",
    "        t1 = time_sync()\n",
    "        if pt or jit or engine:\n",
    "            im = im.to(device, non_blocking=True)\n",
    "            targets = targets.to(device)\n",
    "        im = im.half() if half else im.float()  # uint8 to fp16/32\n",
    "        im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "        nb, _, height, width = im.shape  # batch size, channels, height, width\n",
    "        t2 = time_sync()\n",
    "        dt[0] += t2 - t1\n",
    "\n",
    "        # Inference\n",
    "        out, train_out = model(im) if training else model(im, augment=augment, val=True)  # inference, loss outputs\n",
    "        dt[1] += time_sync() - t2\n",
    "\n",
    "        # Loss\n",
    "        if compute_loss:\n",
    "            loss += compute_loss([x.float() for x in train_out], targets)[1]  # box, obj, cls\n",
    "\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.Tensor([width, height, width, height]).to(device)  # to pixels\n",
    "        lb = [targets[targets[:, 0] == i, 1:] for i in range(nb)] if save_hybrid else []  # for autolabelling\n",
    "        t3 = time_sync()\n",
    "        out = non_max_suppression(out, conf_thres, iou_thres, labels=lb, multi_label=True, agnostic=single_cls)\n",
    "        dt[2] += time_sync() - t3\n",
    "\n",
    "        # Metrics\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            path, shape = Path(paths[si]), shapes[si][0]\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            if single_cls:\n",
    "                pred[:, 5] = 0\n",
    "            predn = pred.clone()\n",
    "            scale_coords(im[si].shape[1:], predn[:, :4], shape, shapes[si][1])  # native-space pred\n",
    "\n",
    "            # Evaluate\n",
    "            if nl:\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])  # target boxes\n",
    "                scale_coords(im[si].shape[1:], tbox, shape, shapes[si][1])  # native-space labels\n",
    "                labelsn = torch.cat((labels[:, 0:1], tbox), 1)  # native-space labels\n",
    "                correct = process_batch(predn, labelsn, iouv)\n",
    "                if plots:\n",
    "                    confusion_matrix.process_batch(predn, labelsn)\n",
    "            else:\n",
    "                correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))  # (correct, conf, pcls, tcls)\n",
    "\n",
    "            # Save/log\n",
    "            if save_txt:\n",
    "                save_one_txt(predn, save_conf, shape, file=save_dir / 'labels' / (path.stem + '.txt'))\n",
    "            if save_json:\n",
    "                save_one_json(predn, jdict, path, class_map)  # append to COCO-JSON dictionary\n",
    "            callbacks.run('on_val_image_end', pred, predn, path, names, im[si])\n",
    "\n",
    "        # Plot images\n",
    "        if plots and batch_i < 3:\n",
    "            f = save_dir / f'val_batch{batch_i}_labels.jpg'  # labels\n",
    "            Thread(target=plot_images, args=(im, targets, paths, f, names), daemon=True).start()\n",
    "            f = save_dir / f'val_batch{batch_i}_pred.jpg'  # predictions\n",
    "            Thread(target=plot_images, args=(im, output_to_target(out), paths, f, names), daemon=True).start()\n",
    "\n",
    "    # Compute metrics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        tp, fp, p, r, f1, ap, ap_class = ap_per_class(*stats, plot=plots, save_dir=save_dir, names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    # Print results\n",
    "    pf = '%20s' + '%11i' * 2 + '%11.3g' * 4  # print format\n",
    "    LOGGER.info(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
    "\n",
    "    # Print results per class\n",
    "    if (verbose or (nc < 50 and not training)) and nc > 1 and len(stats):\n",
    "        for i, c in enumerate(ap_class):\n",
    "            LOGGER.info(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
    "\n",
    "    # Print speeds\n",
    "    t = tuple(x / seen * 1E3 for x in dt)  # speeds per image\n",
    "    if not training:\n",
    "        shape = (batch_size, 3, imgsz, imgsz)\n",
    "        LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {shape}' % t)\n",
    "\n",
    "    # Plots\n",
    "    if plots:\n",
    "        confusion_matrix.plot(save_dir=save_dir, names=list(names.values()))\n",
    "        callbacks.run('on_val_end')\n",
    "\n",
    "    # Save JSON\n",
    "    if save_json and len(jdict):\n",
    "        w = Path(weights[0] if isinstance(weights, list) else weights).stem if weights is not None else ''  # weights\n",
    "        anno_json = str(Path(data.get('path', '../coco')) / 'annotations/instances_val2017.json')  # annotations json\n",
    "        pred_json = str(save_dir / f\"{w}_predictions.json\")  # predictions json\n",
    "        LOGGER.info(f'\\nEvaluating pycocotools mAP... saving {pred_json}...')\n",
    "        with open(pred_json, 'w') as f:\n",
    "            json.dump(jdict, f)\n",
    "\n",
    "        try:  # https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoEvalDemo.ipynb\n",
    "            check_requirements(['pycocotools'])\n",
    "            from pycocotools.coco import COCO\n",
    "            from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "            anno = COCO(anno_json)  # init annotations api\n",
    "            pred = anno.loadRes(pred_json)  # init predictions api\n",
    "            eval = COCOeval(anno, pred, 'bbox')\n",
    "            if is_coco:\n",
    "                eval.params.imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]  # image IDs to evaluate\n",
    "            eval.evaluate()\n",
    "            eval.accumulate()\n",
    "            eval.summarize()\n",
    "            map, map50 = eval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
    "        except Exception as e:\n",
    "            LOGGER.info(f'pycocotools unable to run: {e}')\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    if not training:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n",
    "    maps = np.zeros(nc) + map\n",
    "    for i, c in enumerate(ap_class):\n",
    "        maps[c] = ap[i]\n",
    "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t\n",
    "\n",
    "\n",
    "def parse_opt():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model.pt path(s)')\n",
    "    parser.add_argument('--batch-size', type=int, default=32, help='batch size')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "    parser.add_argument('--conf-thres', type=float, default=0.001, help='confidence threshold')\n",
    "    parser.add_argument('--iou-thres', type=float, default=0.6, help='NMS IoU threshold')\n",
    "    parser.add_argument('--task', default='val', help='train, val, test, speed or study')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "    parser.add_argument('--workers', type=int, default=8, help='max dataloader workers (per RANK in DDP mode)')\n",
    "    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
    "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "    parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n",
    "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "    parser.add_argument('--save-hybrid', action='store_true', help='save label+prediction hybrid results to *.txt')\n",
    "    parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "    parser.add_argument('--save-json', action='store_true', help='save a COCO-JSON results file')\n",
    "    parser.add_argument('--project', default=ROOT / 'runs/val', help='save to project/name')\n",
    "    parser.add_argument('--name', default='exp', help='save to project/name')\n",
    "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\n",
    "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
    "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
    "    opt = parser.parse_args()\n",
    "    opt.data = check_yaml(opt.data)  # check YAML\n",
    "    opt.save_json |= opt.data.endswith('coco.yaml')\n",
    "    opt.save_txt |= opt.save_hybrid\n",
    "    print_args(FILE.stem, opt)\n",
    "    return opt\n",
    "\n",
    "\n",
    "def main(opt):\n",
    "    check_requirements(requirements=ROOT / 'requirements.txt', exclude=('tensorboard', 'thop'))\n",
    "\n",
    "    if opt.task in ('train', 'val', 'test'):  # run normally\n",
    "        if opt.conf_thres > 0.001:  # https://github.com/ultralytics/yolov5/issues/1466\n",
    "            LOGGER.info(f'WARNING: confidence threshold {opt.conf_thres} >> 0.001 will produce invalid mAP values.')\n",
    "        run(**vars(opt))\n",
    "\n",
    "    else:\n",
    "        weights = opt.weights if isinstance(opt.weights, list) else [opt.weights]\n",
    "        opt.half = True  # FP16 for fastest results\n",
    "        if opt.task == 'speed':  # speed benchmarks\n",
    "            # python val.py --task speed --data coco.yaml --batch 1 --weights yolov5n.pt yolov5s.pt...\n",
    "            opt.conf_thres, opt.iou_thres, opt.save_json = 0.25, 0.45, False\n",
    "            for opt.weights in weights:\n",
    "                run(**vars(opt), plots=False)\n",
    "\n",
    "        elif opt.task == 'study':  # speed vs mAP benchmarks\n",
    "            # python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n.pt yolov5s.pt...\n",
    "            for opt.weights in weights:\n",
    "                f = f'study_{Path(opt.data).stem}_{Path(opt.weights).stem}.txt'  # filename to save to\n",
    "                x, y = list(range(256, 1536 + 128, 128)), []  # x axis (image sizes), y axis\n",
    "                for opt.imgsz in x:  # img-size\n",
    "                    LOGGER.info(f'\\nRunning {f} --imgsz {opt.imgsz}...')\n",
    "                    r, _, t = run(**vars(opt), plots=False)\n",
    "                    y.append(r + t)  # results and times\n",
    "                np.savetxt(f, y, fmt='%10.4g')  # save\n",
    "            os.system('zip -r study.zip study_*.txt')\n",
    "            plot_val_study(x=x)  # plot\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt = parse_opt()\n",
    "    main(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d203a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
