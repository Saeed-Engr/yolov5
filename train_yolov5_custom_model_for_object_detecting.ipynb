{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train yolov5 custom model for object detecting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install dependencies"
      ],
      "metadata": {
        "id": "65AcNsX-Wc_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4sDLDpeWWyF",
        "outputId": "a76d789f-850b-49c3-b481-d7725b4feef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: pip3 [options]\n",
            "\n",
            "\u001b[31mERROR: Invalid requirement: -python>=4.1.2\n",
            "pip3: error: no such option: -p\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yWDYhdW6aL64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Training with YOLOv5\n",
        "\n",
        "In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
        "\n",
        "* Gather a dataset of images and label our dataset\n",
        "* Export our dataset to YOLOv5\n",
        "* Train YOLOv5 to recognize the objects in our dataset\n",
        "* Evaluate our YOLOv5 model's performance\n",
        "* Run test inference to view our model at work"
      ],
      "metadata": {
        "id": "2Ba_M0sTaSQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yQPj4wEzVovs",
        "outputId": "dd75e03e-a48f-4b4b-abc6-6758a3787958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/yolov5 model/yolov5 - Copy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "6z9wQvTuXojG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set the path of pre-train model"
      ],
      "metadata": {
        "id": "F5GQxIkcV2ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/yolov5 model/yolov5 - Copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ40CkjoVvoZ",
        "outputId": "122f4703-ced6-47c8-d3d9-bcfd524d9565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/yolov5 model/yolov5 - Copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ],
      "metadata": {
        "id": "yY-7Ojvha3Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 416 --batch 10 --epochs 15 --data data/coco128.yaml --weights yolov5s.pt --cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne-WifVQWKgj",
        "outputId": "20e72eb2-8f93-4362-d294-65206db03bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=data/coco128.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=15, batch_size=10, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\n",
            "Parse error at \"'-'\": Expected W:(abcd...)\n",
            "YOLOv5 ðŸš€ 2022-1-24 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 ðŸš€ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     29667  models.yolo.Detect                      [6, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 283 layers, 7077027 parameters, 7077027 gradients\n",
            "\n",
            "Transferred 355/361 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight (no decay), 62 weight, 62 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/yolov5 model/data/labels/train.cache' images and labels... 125 found, 0 missing, 0 empty, 0 corrupt: 100% 125/125 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 125/125 [00:17<00:00,  7.12it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/yolov5 model/data/labels/val.cache' images and labels... 34 found, 0 missing, 0 empty, 0 corrupt: 100% 34/34 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100% 34/34 [00:15<00:00,  2.20it/s]\n",
            "Plotting labels to runs/train/exp16/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.73 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp16\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/14    0.965G    0.1215   0.02393   0.05627        11       416: 100% 13/13 [00:10<00:00,  1.23it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.09it/s]\n",
            "                 all         34         63     0.0368      0.133     0.0403    0.00805\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/14      1.1G    0.1167   0.02769   0.05677        22       416: 100% 13/13 [00:08<00:00,  1.53it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.18it/s]\n",
            "                 all         34         63     0.0459     0.0844     0.0273    0.00504\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/14      1.1G    0.1079   0.02521   0.05249        16       416: 100% 13/13 [00:08<00:00,  1.53it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.26it/s]\n",
            "                 all         34         63     0.0364      0.067     0.0258    0.00581\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/14      1.1G    0.1029   0.02833   0.05162        14       416: 100% 13/13 [00:08<00:00,  1.53it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.36it/s]\n",
            "                 all         34         63     0.0379     0.0918     0.0275    0.00693\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/14      1.1G   0.09149   0.02858   0.05087        16       416: 100% 13/13 [00:08<00:00,  1.53it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.46it/s]\n",
            "                 all         34         63     0.0513       0.11     0.0479     0.0145\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/14      1.1G   0.08598    0.0328   0.04712        13       416: 100% 13/13 [00:08<00:00,  1.53it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.52it/s]\n",
            "                 all         34         63     0.0922     0.0757     0.0544     0.0121\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/14      1.1G   0.08293   0.03223   0.04805        17       416: 100% 13/13 [00:08<00:00,  1.54it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.49it/s]\n",
            "                 all         34         63     0.0718      0.243     0.0735      0.017\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/14      1.1G   0.08528   0.03387   0.04522        16       416: 100% 13/13 [00:08<00:00,  1.56it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.53it/s]\n",
            "                 all         34         63     0.0977      0.226     0.0979     0.0231\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/14      1.1G   0.08289   0.03433   0.04542        16       416: 100% 13/13 [00:08<00:00,  1.56it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.67it/s]\n",
            "                 all         34         63      0.137      0.347      0.144     0.0435\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/14      1.1G   0.08018   0.03594   0.04454        20       416: 100% 13/13 [00:08<00:00,  1.58it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.64it/s]\n",
            "                 all         34         63       0.24       0.19      0.207     0.0667\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/14      1.1G   0.07561   0.03508    0.0441        27       416: 100% 13/13 [00:08<00:00,  1.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.58it/s]\n",
            "                 all         34         63      0.462      0.168      0.262     0.0975\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/14      1.1G   0.07528   0.03499   0.04525        15       416: 100% 13/13 [00:08<00:00,  1.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.63it/s]\n",
            "                 all         34         63       0.31      0.321      0.307      0.121\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/14      1.1G   0.07356   0.03373   0.04155        20       416: 100% 13/13 [00:08<00:00,  1.56it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.70it/s]\n",
            "                 all         34         63      0.288      0.357      0.328      0.147\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/14      1.1G   0.07066    0.0347    0.0428        25       416: 100% 13/13 [00:08<00:00,  1.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.72it/s]\n",
            "                 all         34         63      0.197      0.445      0.315      0.134\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/14      1.1G   0.07249   0.03875   0.03973        23       416: 100% 13/13 [00:08<00:00,  1.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:00<00:00,  2.48it/s]\n",
            "                 all         34         63      0.246      0.348      0.279      0.117\n",
            "\n",
            "15 epochs completed in 0.042 hours.\n",
            "Optimizer stripped from runs/train/exp16/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp16/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp16/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 224 layers, 7067395 parameters, 0 gradients\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 2/2 [00:01<00:00,  1.28it/s]\n",
            "                 all         34         63      0.288      0.356      0.328      0.147\n",
            "              person         34         27          0          0     0.0428     0.0109\n",
            "          cell phone         34          5      0.186        0.4      0.314      0.129\n",
            "              bottle         34         23      0.596      0.609      0.561      0.165\n",
            "           ballpoint         34          3          0          0     0.0573     0.0338\n",
            "              folder         34          5      0.658      0.773      0.666      0.396\n",
            "Results saved to \u001b[1mruns/train/exp16\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VW3mioyYXt-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ],
      "metadata": {
        "id": "Y1uSyAIPbn6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source {dataset.location}/test/images"
      ],
      "metadata": {
        "id": "7gxGR5ATbpWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TGWEsKE4eVM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}